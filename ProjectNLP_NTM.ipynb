{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "ProjectNLP_NTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NLPF8jJTpyxM"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "mVifDCldn8o8"
      },
      "source": [
        "# Neural Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "nbIOa46cn8pA"
      },
      "source": [
        "##  English version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "u0UrEQcXn8pB"
      },
      "source": [
        "First of all, we can check how much GPU we have access to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sflgxsKxs2Zw",
        "gather": {
          "logged": 1624871792291
        }
      },
      "source": [
        "import GPUtil\n",
        "GPUs = GPUtil.getGPUs()\n",
        "for i, gpu in enumerate(GPUs):\n",
        "  print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "GrSC_NDQn8pE"
      },
      "source": [
        "During the first run, we clone the repository kindly made public by the authors of the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_w9cosIoz6N"
      },
      "source": [
        "# !git clone https://github.com/ahoho/kd-topic-models.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "evfS8xS4n8pF"
      },
      "source": [
        "Now, let's create the environments for the teacher and the student."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJDYNjBsp1QA"
      },
      "source": [
        "!conda env create -f /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/teacher/teacher.yml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCap0jd4qJvu"
      },
      "source": [
        "!conda env create -f /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/scholar/scholar.yml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "iuWsQDqRn8pH"
      },
      "source": [
        "### Preprocessing 20NG dataset\n",
        "We perform the same preprocessing steps as the authors of the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "eteZ_BVmn8pH"
      },
      "source": [
        "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/1_convert_prodlda_to_txt_py27.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "D_W4rKUxn8pI"
      },
      "source": [
        "! python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/2_convert_txt_to_scholar_format_py3.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "7WZ_61Gwn8pI"
      },
      "source": [
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/3_replicate_and_align_raw_data.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "ru7VbAOMn8pJ"
      },
      "source": [
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/4_create_dev_sets.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "4tEAqE5kn8pJ"
      },
      "source": [
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/5_create_aligned_dev_set.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "po2izU2dn8pJ"
      },
      "source": [
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/6_create_raw_text_file.py /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/replicated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "wH7hnCbHn8pK"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "wK68hITnn8pK"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from gensim.corpora.dictionary import Dictionary \n",
        "nltk.download(\"stopwords\") \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_documents(dataset, dictionary = None, language = 'english'):\n",
        "\n",
        "    \"\"\"Preprocess the dataset to obtain the input for the model.\n",
        "    Parameters\n",
        "    ----------\n",
        "    - dataset: List of Strings (each String is a document of the dataset)\n",
        "    - dictionary: Dictionary, default None (for the training, set it to None in order to extract the dictionary of the words from the corpus. \n",
        "                                            During test give the dictionary computed for the training.)\n",
        "    - language: String, default \"english\" (language of the dataset)\n",
        "    Output\n",
        "    ------\n",
        "    - dictionary: Dictionary (dictionary containing all the words of the corpus, each associated to an int key.\n",
        "                              If the input dictionary is not None, the input dictionary is returned.)\n",
        "    - corpus: List of Lists of Tuples (each document is mapped to tuples at a word-level. Each tuple contains two elements, \n",
        "                                       the int-key of the word in the dictionary and the number of times it appears in the specific document considered.)\n",
        "    - lemmatized_words: List of Lists of Strings (contains the lemmatized words)\"\"\"\n",
        "    \n",
        "\n",
        "    ### Tokenizer\n",
        "    \n",
        "    tokenized = []\n",
        "    for doc in dataset:\n",
        "        tokenized_doc = word_tokenize(doc)\n",
        "        tokenized_doc= [word.lower() for word in tokenized_doc if word.isalpha()]\n",
        "        tokenized.append(tokenized_doc)\n",
        "\n",
        "    ### Stopwords\n",
        "\n",
        "    if language == \"italian\": # added some stopwords specific for the webhose corpus in italian\n",
        "        stop_words = nltk.corpus.stopwords.words(language)\n",
        "        newStopWords = ['nflash','credits','ansa', 'gen', 'feb', 'mar', 'apr', 'mag', 'giu', 'lug', 'ago', 'set', 'sett', 'ott', 'nov', 'dic']\n",
        "        stop_words.extend(newStopWords)\n",
        "    \n",
        "    elif language == \"english\":\n",
        "        stop_words = set(stopwords.words(language))\n",
        "    \n",
        "    filtered_list = []\n",
        "    for doc in tokenized :\n",
        "        filtered_doc = [word for word in doc if word.casefold() not in stop_words and re.match(r'.*([a-zA-Z])\\1{3,}',word) is None]\n",
        "        filtered_list.append(filtered_doc)\n",
        "    ### Lemmatizing\n",
        "    if language == \"english\":\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_words = []\n",
        "        for doc in filtered_list:\n",
        "            lemmatized_doc = [lemmatizer.lemmatize(word) for word in doc]\n",
        "            lemmatized_words.append(lemmatized_doc)\n",
        "    elif language == \"italian\":\n",
        "        lemmatized_words = italian_lemmatizer(filtered_list)\n",
        "\n",
        "    if dictionary is None:\n",
        "        dictionary = Dictionary(lemmatized_words)\n",
        "    \n",
        "    corpus = [dictionary.doc2bow(text) for text in lemmatized_words]\n",
        "    \n",
        "    return dictionary, corpus, lemmatized_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "g6wPjBUdn8pL"
      },
      "source": [
        "from gensim.models import LdaModel\n",
        "import numpy as np\n",
        "\n",
        "def apply_lda_and_save_topics_file(out_file_path, num_topics, train_corpus, train_dict):\n",
        "    \n",
        "    \"\"\"Instantiate an LDA model and save the computed topics in a txt file.\n",
        "    Parameters\n",
        "    ----------\n",
        "    - out_file_path: String\n",
        "    - num_topics: int (number of topics to be computed by LDA)\n",
        "    - train_corpus: List of Lists of Strings (every document is represented by a list of words, and the corpus is a list of all the documents)\n",
        "    - train_dict: Dictionary (dictionary containing all the words of the corpus, each associated to an int key)\n",
        "    Output\n",
        "    ------\n",
        "    - topics: List of Strings (each String is a topic. The length of the list is num_topics)\"\"\"\n",
        "\n",
        "    lda = LdaModel(train_corpus, num_topics=num_topics, id2word=train_dict, dtype=np.float64, passes = 100)\n",
        "    topics  =[]\n",
        "    with open(out_file_path,\"w\") as f:\n",
        "        for element in lda.show_topics(num_topics = num_topics, formatted = False):\n",
        "            current_topics = \"\"\n",
        "            for value in element[1]:\n",
        "                current_topics = current_topics + ' ' + str(value[0])\n",
        "            topics.append(current_topics.strip())\n",
        "            f.write(current_topics + \"\\n\")\n",
        "    return topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "qWL1oFywn8pL"
      },
      "source": [
        "Obtain the 20NG dataset from sklearn and preprocess it with the above functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Mp6U2wvxn8pM"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_dict, train_corpus, train_text = preprocess_documents(newsgroups_train.data)\n",
        "test_dict, test_corpus, test_text = preprocess_documents(newsgroups_test.data, train_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "bO5yYVjUn8pM"
      },
      "source": [
        "k = 50 # number_topics\n",
        "path_lda_topics = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/outputs/LDA{}_ENG/topics.txt\".format(k)\n",
        "apply_lda_and_save_topics_file(path_lda_topics, k, train_corpus, train_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "xfpRfDVxn8pN"
      },
      "source": [
        "#### Internal NPMI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "fgZa56yvn8pN"
      },
      "source": [
        "import codecs\n",
        "from scipy import sparse\n",
        "\n",
        "def read_text(input_filename):\n",
        "    with codecs.open(input_filename, 'r', encoding='utf-8') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "    return lines\n",
        "def read_json(input_filename):\n",
        "    with open(input_filename, 'r', encoding='utf-8') as input_file:\n",
        "        data = json.load(input_file, encoding='utf-8')\n",
        "    return data\n",
        "def load_sparse(input_filename):\n",
        "    npy = np.load(input_filename)\n",
        "    coo_matrix = sparse.coo_matrix((npy['data'], (npy['row'], npy['col'])), shape=npy['shape'])\n",
        "    return coo_matrix.tocsc()\n",
        "\n",
        "def write_list_to_text(lines, output_filename, add_newlines=True, add_final_newline=False):\n",
        "    with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
        "        for i, line in enumerate(lines):\n",
        "            output_file.write(line)\n",
        "            if add_newlines and i < len(lines) - 1:\n",
        "                output_file.write('\\n')\n",
        "        \n",
        "        if add_final_newline:\n",
        "            output_file.write('\\n')\n",
        "\n",
        "def load_and_compute_npmi(topics_file, ref_vocab_file, ref_counts_file, n_vals, cols_to_skip=0, output_file=None):\n",
        "    print(\"Loading reference counts\")\n",
        "    ref_vocab = read_json(ref_vocab_file)\n",
        "    ref_counts = load_sparse(ref_counts_file).tocsc()\n",
        "    compute_npmi(topics_file, ref_vocab, ref_counts, n_vals, cols_to_skip, output_file)\n",
        "\n",
        "\n",
        "def compute_npmi(topics_file, ref_vocab, ref_counts, n_vals, cols_to_skip=0, output_file=None):\n",
        "    print(\"Loading topics\")\n",
        "    topics = read_text(topics_file)\n",
        "\n",
        "    mean_vals = []\n",
        "    for n in range(n_vals):\n",
        "        mean_npmi = compute_npmi_at_n(topics, ref_vocab, ref_counts, n, cols_to_skip=cols_to_skip)\n",
        "        mean_vals.append(mean_npmi)\n",
        "\n",
        "    if output_file is not None:\n",
        "        lines = [str(n) + ' ' + str(v) for n, v in zip(range(n_vals), mean_vals)]\n",
        "        write_list_to_text(lines, output_file)\n",
        "\n",
        "\n",
        "def compute_npmi_at_n(\n",
        "    topics, ref_vocab, ref_counts, n=10, cols_to_skip=0, silent=False, return_mean=True\n",
        "):\n",
        "\n",
        "    vocab_index = dict(zip(ref_vocab, range(len(ref_vocab))))\n",
        "    n_docs, _ = ref_counts.shape\n",
        "\n",
        "    npmi_means = []\n",
        "    for topic in topics:\n",
        "        words = topic.strip().split()[cols_to_skip:]\n",
        "        npmi_vals = []\n",
        "        for word_i, word1 in enumerate(words[:n]):\n",
        "            if word1 in vocab_index:\n",
        "                index1 = vocab_index[word1]\n",
        "            else:\n",
        "                index1 = None\n",
        "            for word2 in words[word_i+1:n]:\n",
        "                if word2 in vocab_index:\n",
        "                    index2 = vocab_index[word2]\n",
        "                else:\n",
        "                    index2 = None\n",
        "                if index1 is None or index2 is None:\n",
        "                    npmi = 0.0\n",
        "                else:\n",
        "                    col1 = np.array(ref_counts[:, index1].todense() > 0, dtype=int)\n",
        "                    col2 = np.array(ref_counts[:, index2].todense() > 0, dtype=int)\n",
        "                    c1 = col1.sum()\n",
        "                    c2 = col2.sum()\n",
        "                    c12 = np.sum(col1 * col2)\n",
        "                    if c12 == 0:\n",
        "                        npmi = 0.0\n",
        "                    else:\n",
        "                        npmi = (np.log10(n_docs) + np.log10(c12) - np.log10(c1) - np.log10(c2)) / (np.log10(n_docs) - np.log10(c12))\n",
        "                npmi_vals.append(npmi)\n",
        "        if not silent:\n",
        "            print(str(np.mean(npmi_vals)) + ': ' + ' '.join(words[:n]))\n",
        "        npmi_means.append(np.mean(npmi_vals))\n",
        "    if not silent:\n",
        "        print(np.mean(npmi_means))\n",
        "    if return_mean:\n",
        "        return np.mean(npmi_means)\n",
        "    else:\n",
        "        return np.array(npmi_means)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "fhlAZob2n8pO"
      },
      "source": [
        "topics_file = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/outputs/LDA50_ENG/topics.txt\"\n",
        "ref_vocab_file = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/20ng/replicated/train.vocab.json\"\n",
        "ref_counts_file = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/20ng/replicated/test.npz\"\n",
        "load_and_compute_npmi(topics_file, ref_vocab_file, ref_counts_file, 10, output_file=\"internal_npmi_LDA_k50_ENG.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "a1fneIxsn8pO"
      },
      "source": [
        "### Scholar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628944184863
        },
        "id": "1UQzB8Ynn8pO"
      },
      "source": [
        "!source activate scholar\n",
        "\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/scholar/run_scholar.py \\\n",
        "    /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/replicated/dev \\\n",
        "    --dev-metric npmi \\\n",
        "    -k 200 \\\n",
        "    --epochs 500 \\\n",
        "    --patience 450 \\\n",
        "    --batch-size 200 \\\n",
        "    --background-embeddings \\\n",
        "    --device 0 \\\n",
        "    --dev-prefix dev \\\n",
        "    -l 0.002 \\\n",
        "    --alpha 1.0 \\\n",
        "    --eta-bn-anneal-step-const 0.25 \\\n",
        "    --use-doc-layer \\\n",
        "    -o ./outputs/20ng/scholar_k200_defaultParameters/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "GjPZSTyxn8pP"
      },
      "source": [
        "### Scholar + BAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "2SBCKNiOn8pQ"
      },
      "source": [
        "#### Teacher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "vJsLHcmrn8pQ"
      },
      "source": [
        "# !source activate transformers28\n",
        "\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/teacher/bert_reconstruction.py \\\n",
        "--input-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/replicated/dev \\\n",
        "--output-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/replicated/dev/logits \\\n",
        "--do-train \\\n",
        "--evaluate-during-training \\\n",
        "--save-steps 112 \\\n",
        "--logging-steps 112 \\\n",
        "--num-train-epochs 6 \\\n",
        "--seed 42 \\\n",
        "--num-workers 4 \\\n",
        "--batch-size 10 \\\n",
        "--gradient-accumulation-steps 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "NFB3uOXMn8pQ"
      },
      "source": [
        "Extract the logits from the trained teacher model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lJVhJpW1UG-",
        "gather": {
          "logged": 1629191262576
        }
      },
      "source": [
        "#!source activate transformers28\n",
        "\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/teacher/bert_reconstruction.py \\\n",
        "    --output-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/replicated/dev/logits \\\n",
        "    --seed 42 \\\n",
        "    --num-workers 4 \\\n",
        "    --get-reps \\\n",
        "    --checkpoint-folder-pattern \"checkpoint-672\" \\\n",
        "    --save-doc-logits \\\n",
        "    --no-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "gDHlvtUQn8pQ"
      },
      "source": [
        "#### Student"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy73K75r1Yeh",
        "gather": {
          "logged": 1627121270241
        }
      },
      "source": [
        "!source activate scholar\n",
        "\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/scholar/run_scholar.py \\\n",
        "    /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/replicated/dev \\\n",
        "    --dev-metric npmi \\\n",
        "    -k 50 \\\n",
        "    --epochs 500 \\\n",
        "    --patience 500 \\\n",
        "    --batch-size 200 \\\n",
        "    --background-embeddings \\\n",
        "    --device 0 \\\n",
        "    --dev-prefix dev \\\n",
        "    -l 0.002 \\\n",
        "    --alpha 1.0 \\\n",
        "    --eta-bn-anneal-step-const 0.25 \\\n",
        "    --doc-reps-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/20ng/replicated/dev/logits/checkpoint-672/doc_logits \\\n",
        "    --use-doc-layer \\\n",
        "    --no-bow-reconstruction-loss \\\n",
        "    --doc-reconstruction-weight 0.75 \\\n",
        "    --doc-reconstruction-temp 2.0 \\\n",
        "    --doc-reconstruction-logit-clipping 10.0 \\\n",
        "    -o ./outputs/20ng/k50_optimalParameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "pRu2DwUTn8pR"
      },
      "source": [
        "### Palmetto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625069538036
        },
        "id": "abYKhqADn8pR"
      },
      "source": [
        "from palmettopy.palmetto import Palmetto\n",
        "import requests\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def evaluate_with_palmetto(path_to_topics, n_words_topic, coherence_measure=\"npmi\", output_file = None):\n",
        "  \"\"\"Evaluate the topics with the Palmetto framework.\n",
        "  Parameters\n",
        "  ----------\n",
        "  - path_to_topics: String (path the the txt file containing the topics extracted by the student)\n",
        "  - n_words_topics: int (numer of words to consider for each topic)\n",
        "  - coherence_measure: String, default \"npmi\" (coherence measure to be used for the evaluation. The available measures are: ca, cp, cv, npmi, uci, umass)\n",
        "  - output_file: String, default None (txt file where to save the NPMI measure for each topic and the average one considering all the topics)\n",
        "  Output\n",
        "  ------\n",
        "  - avg_npmi: float (average NPMI considering all the topics)\"\"\"\n",
        "  \n",
        "  palmetto = Palmetto()\n",
        "  coherence_values = []\n",
        "  with open(path_to_topics,\"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    if output_file is not None:\n",
        "      g = open(output_file,\"w\")\n",
        "    for topic in lines:\n",
        "      words = topic.split(\" \")[:n_words_topic+1]\n",
        "      words_string = \"\"\n",
        "      for word in words:\n",
        "          words_string = words_string + word + \" \"\n",
        "      words_string = words_string.strip()\n",
        "      print(words_string)\n",
        "      # coherence_values.append(palmetto.get_coherence(words, coherence_type=coherence_measure))\n",
        "      r =requests.get(\"https://palmetto.demos.dice-research.org/service/{}?words={}\".format(coherence_measure,words_string))\n",
        "      print(r.text)\n",
        "\n",
        "      if re.match(r'^-?\\d+(?:\\.\\d+)$', r.text) is not None:\n",
        "        coherence_values.append(float(r.text))\n",
        "      else :\n",
        "        print(str(r.status_code) + \" - \" + r.text + \"[\" + words_string + \"]\")\n",
        "\n",
        "      if output_file is not None:\n",
        "        g.write(words_string + \" --> NPMI: \" + r.text + \"\\n\")\n",
        "    avg_npmi = np.mean(coherence_values)\n",
        "    if output_file is not None:\n",
        "        g.write(\"\\n\\nAVERAGE NPMI: \" + str(avg_npmi))\n",
        "        g.close()\n",
        "    return avg_npmi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "vdCCEYPNn8pR"
      },
      "source": [
        "evaluate_with_palmetto(\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/outputs/20ng/k50_defaultParameters/topics.txt\",10,output_file= \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/outputs/20ng/k50_defaultParameters/topics_npmi.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "5C4LKAsYn8pS"
      },
      "source": [
        "## Italian version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "JBgkutUtn8pS"
      },
      "source": [
        "First of all we will obtain the italian lemmatizer which will be used to preprocess the documents of the italian corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "01aEWHkxn8pS"
      },
      "source": [
        "!pip install treetaggerwrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "-v7JyxJLn8pS"
      },
      "source": [
        "%%bash\n",
        "mkdir treetagger\n",
        "cd treetagger\n",
        "# Download the tagger package for your system (PC-Linux, Mac OS-X, ARM64, ARMHF, ARM-Android, PPC64le-Linux).\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tree-tagger-linux-3.2.4.tar.gz\n",
        "tar -xzvf tree-tagger-linux-3.2.4.tar.gz\n",
        "# Download the tagging scripts into the same directory.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tagger-scripts.tar.gz\n",
        "gunzip tagger-scripts.tar.gz\n",
        "# Download the installation script install-tagger.sh.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/install-tagger.sh\n",
        "# Download the parameter files for the languages you want to process.\n",
        "# list of all files (parameter files) https://cis.lmu.de/~schmid/tools/TreeTagger/#parfiles\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/italian.par.gz\n",
        "sh install-tagger.sh\n",
        "cd ..\n",
        "#sudo pip install treetaggerwrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "uW3cNdDwn8pS"
      },
      "source": [
        "### Preprocessing Webhose dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630575697367
        },
        "id": "_nbLVL6mn8pS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from scipy import sparse\n",
        "import json\n",
        "import treetaggerwrapper\n",
        "\n",
        "def italian_lemmatizer (tokenized_corpus):\n",
        "  # tokenized_corpus list of lists of strings\n",
        "  corpus = []\n",
        "  for doc in tokenized_corpus:\n",
        "    current_str = \"\"\n",
        "    for word in doc :\n",
        "      current_str = current_str + ' ' + str(word)\n",
        "    corpus.append(current_str.strip())\n",
        "  tagger = treetaggerwrapper.TreeTagger(TAGLANG='it', TAGDIR='treetagger/') \n",
        "  lemmatized = [] \n",
        "  for text in corpus:\n",
        "    tags = tagger.tag_text(text)\n",
        "    lemmatized.append([tag.split('\\t')[2] for tag in tags])\n",
        "  return lemmatized\n",
        "  \n",
        "def toks_to_onehot(doc, vocab):\n",
        "    tokens =  [vocab[word] for word in doc if word in vocab]                         \n",
        "    return np.bincount(tokens, minlength=len(vocab))\n",
        "    \n",
        "def save_sparse(sparse_matrix, output_filename):\n",
        "    assert sparse.issparse(sparse_matrix)\n",
        "    if sparse.isspmatrix_coo(sparse_matrix):\n",
        "        coo = sparse_matrix\n",
        "    else:\n",
        "        coo = sparse_matrix.tocoo()\n",
        "    row = coo.row\n",
        "    col = coo.col\n",
        "    data = coo.data\n",
        "    shape = coo.shape\n",
        "    np.savez(output_filename, row=row, col=col, data=data, shape=shape)\n",
        "\n",
        "def save_json(obj, fpath):\n",
        "    with open(fpath, 'w') as o:\n",
        "        json.dump(obj, o, ensure_ascii=False)\n",
        "\n",
        "def save_jsonlist(dicts, fpath):\n",
        "    with open(fpath, 'w', encoding='utf-8') as o:\n",
        "        for d in dicts:\n",
        "            json.dump(d, o)\n",
        "            o.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630575636622
        },
        "id": "_izDoXZUn8pT"
      },
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "import pickle\n",
        "def preprocess_dataset_webhose(corpus_path):\n",
        "\n",
        "    \"\"\"Preprocess the italian dataset to obtain the input for the model.\n",
        "    Parameters\n",
        "    ----------\n",
        "    - corpus_path: String, the path of the .txt file containig the corpus where each document is represented by a JSON file.\n",
        "    Output\n",
        "    ------\n",
        "    - train_dict: Dictionary, the dictionary created on the trainig dataset.\n",
        "    - train_corpus: List of Lists of Tuples, the training dataset in an appropriate format for the LDA model\"\"\"\n",
        "    \n",
        "\n",
        "    docs = []\n",
        "    with open(corpus_path,\"r\") as file_reader:\n",
        "        corpus = file_reader.read()\n",
        "        json_lists = json.loads(\"[{}]\".format(corpus.replace('}{', '},{')))\n",
        "    for doc in tqdm(json_lists):\n",
        "        docs.append(doc[\"text\"])\n",
        "    \n",
        "\n",
        "    \n",
        "    # Train-test split\n",
        "    TEST_SIZE = 0.1\n",
        "    raw_train, raw_test = train_test_split(docs, test_size = TEST_SIZE, random_state = 42, shuffle = True)\n",
        "\n",
        "    # Lemmatize only trainig dataset\n",
        "    train_dict, train_corpus, raw_tokens_train = preprocess_documents(raw_train, language = \"italian\")\n",
        "    \n",
        "    # Swap key and value of the dictionary\n",
        "    vocab_dict = dict([(value, key) for key, value in train_dict.items()])\n",
        "    \n",
        "\n",
        "    # Compute count matrix of training dataset only\n",
        "    raw_counts_train = np.array([toks_to_onehot(doc, vocab_dict) for doc in raw_tokens_train]) \n",
        "\n",
        "    # Our addition: we map each document on a list of ids of the respective words in that document, then we perform the bin count. \n",
        "    #               After that we compute a mask by filtering out the columns relative to that words that do not respect the condition.\n",
        "    mapping_bool = lambda x: int(bool(x))\n",
        "    func = np.vectorize(mapping_bool)\n",
        "    b = func(raw_counts_train)\n",
        "    mask = (np.sum(b,axis = 0) > 15) & (np.sum(b,axis = 0) < 6890) # at least 15 documents and less than 65% \n",
        "    \n",
        "    # New dictionary without the previously removed words\n",
        "    vocab_dict_keys = [k for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])]\n",
        "    new_words = np.array(vocab_dict_keys)[mask]\n",
        "    train_dict = Dictionary([list(new_words)]) \n",
        "    vocab_dict = dict([(value, key) for key, value in train_dict.items()])\n",
        "\n",
        "    # Lemmatize training and test sets\n",
        "    train_dict, train_corpus, raw_tokens_train = preprocess_documents(raw_train, train_dict, language = \"italian\")\n",
        "    test_dict, _, raw_tokens_test = preprocess_documents(raw_test, train_dict, language = \"italian\")\n",
        "\n",
        "    # NOTE: using Dictionary() we shuffle words, we need to recompute the training data set's count matrix\n",
        "    raw_counts_train = np.array([toks_to_onehot(doc, vocab_dict) for doc in raw_tokens_train])\n",
        "    raw_counts_test = np.array([toks_to_onehot(doc, vocab_dict) for doc in raw_tokens_test])\n",
        "\n",
        "    ## Filter out the zero-counts\n",
        "    nonzero_train = raw_counts_train.sum(1) > 0\n",
        "    nonzero_test = raw_counts_test.sum(1) > 0\n",
        "        \n",
        "    ## Keep only non empty documents\n",
        "    raw_ids_train = [idx for idx, keep in enumerate(nonzero_train) if keep] # non empty documents ids\n",
        "    raw_ids_test = [idx for idx, keep in enumerate(nonzero_test) if keep] # list of integers\n",
        "\n",
        "    raw_tokens_train = [' '.join(raw_tokens_train[idx]) for idx in raw_ids_train] # non empty documents lemmatized\n",
        "    raw_tokens_test = [' '.join(raw_tokens_test[idx]) for idx in raw_ids_test] # list of strings\n",
        "\n",
        "    raw_counts_train = raw_counts_train[nonzero_train] # non empty documents word counts\n",
        "    raw_counts_test = raw_counts_test[nonzero_test] # list of lists of 0/1\n",
        "\n",
        "    raw_data_train = [{'id': idx, 'text': raw_train[idx]} for idx in raw_ids_train] # list of dictinaries, one dict for each non empty documents\n",
        "    raw_data_test = [{'id': idx, 'text': raw_test[idx]} for idx in raw_ids_test]\n",
        "\n",
        "    ## save data\n",
        "    %cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose\n",
        "    Path(\"replicated\").mkdir(exist_ok=True)\n",
        "    save_sparse(sparse.coo_matrix(raw_counts_train), \"./replicated/train.npz\")\n",
        "    save_sparse(sparse.coo_matrix(raw_counts_test), \"./replicated/test.npz\")\n",
        "    keys_dict = [\n",
        "        k for k, v in vocab_dict.items()\n",
        "    ]   \n",
        "    save_json(keys_dict, \"./replicated/train.vocab.json\")\n",
        "\n",
        "    save_json(raw_tokens_train, \"./replicated/train.tokens.json\")\n",
        "    save_json(raw_tokens_test, \"./replicated/test.tokens.json\")\n",
        "\n",
        "    save_jsonlist(raw_data_train, \"./replicated/train.jsonlist\")\n",
        "    save_jsonlist(raw_data_test, \"./replicated/test.jsonlist\")\n",
        "\n",
        "    save_json([d['id'] for d in raw_data_train], \"./replicated/train.ids.json\")\n",
        "    save_json([d['id'] for d in raw_data_test], \"./replicated/test.ids.json\")\n",
        "    with open(\"./webhose_corpus.txt\", \"wb\") as internal_filename:\n",
        "        pickle.dump(train_corpus, internal_filename)\n",
        "    return train_dict, train_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "_GIO_Ltbn8pT"
      },
      "source": [
        "We can now perform the preprocessing with the defined functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630576439849
        },
        "id": "j_DewySYn8pT"
      },
      "source": [
        "train_dict_webhose, train_corpus_webhose = preprocess_dataset_webhose(\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/webhose_corpus_clean_new.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "9LJk_GLon8pT"
      },
      "source": [
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/4_create_dev_sets.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "qOkGe6qKn8pT"
      },
      "source": [
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/6_create_raw_text_file.py /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated\n",
        "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "4Fdy9fHsn8pU"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630579629760
        },
        "id": "H746weaPn8pU"
      },
      "source": [
        "apply_lda_and_save_topics_file(\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/LDA/Ita_LDA_topics.txt\", 50, train_corpus_webhose, train_dict_webhose)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "4VZQH7wpn8pU"
      },
      "source": [
        "#### Internal NPMI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630580546277
        },
        "id": "WaxBiN3An8pU"
      },
      "source": [
        "topics_file = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/LDA/Ita_LDA_topics.txt\"\n",
        "ref_vocab_file = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/train.vocab.json\"\n",
        "ref_counts_file = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/test.npz\"\n",
        "load_and_compute_npmi(topics_file, ref_vocab_file, ref_counts_file, 10, output_file=\"internal_npmi_LDA_k50.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "0tkHM1NHn8pU"
      },
      "source": [
        "### Scholar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "0eiDKbKEn8pV"
      },
      "source": [
        "We can test the Scholar model without the application of knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "JW9msNzLn8pV"
      },
      "source": [
        "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630066205988
        },
        "id": "qECHtOwOn8pW"
      },
      "source": [
        "!source activate scholar\n",
        "\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/scholar/run_scholar.py \\\n",
        "    /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/dev \\\n",
        "    --dev-metric npmi \\\n",
        "    -k 50 \\\n",
        "    --epochs 500 \\\n",
        "    --patience 250 \\\n",
        "    --batch-size 200 \\\n",
        "    --background-embeddings \\\n",
        "    --device 0 \\\n",
        "    --dev-prefix dev \\\n",
        "    -l 0.001 \\\n",
        "    --alpha 0.5 \\\n",
        "    --eta-bn-anneal-step-const 0.25 \\\n",
        "    --use-doc-layer \\\n",
        "    -o ./outputs/webhose/scholar_k50_defaultParameters_lr001/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "o2zAID8Un8pW"
      },
      "source": [
        "### Scholar + BAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "BS_-aWggn8pW"
      },
      "source": [
        "#### Teacher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629486958689
        },
        "id": "y22OJ8Ikn8pW"
      },
      "source": [
        "# !source activate transformers28\n",
        "\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/teacher/bert_reconstruction.py \\\n",
        "--input-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/dev  \\\n",
        "--output-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/dev/logits \\\n",
        "--bert-model dbmdz/bert-base-italian-xxl-uncased \\\n",
        "--do-train \\\n",
        "--evaluate-during-training \\\n",
        "--save-steps 165 \\\n",
        "--logging-steps 165 \\\n",
        "--num-train-epochs 8 \\\n",
        "--seed 42 \\\n",
        "--num-workers 4 \\\n",
        "--batch-size 8 \\\n",
        "--gradient-accumulation-steps 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "8437FOJ4n8pW"
      },
      "source": [
        "We can now extract the logits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629524700652
        },
        "id": "MNrixyZ_n8pX"
      },
      "source": [
        "#!source activate transformers28\n",
        "\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/teacher/bert_reconstruction.py \\\n",
        "    --output-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/dev/logits \\\n",
        "    --seed 42 \\\n",
        "    --num-workers 4 \\\n",
        "    --get-reps \\\n",
        "    --checkpoint-folder-pattern \"checkpoint-1155\" \\\n",
        "    --save-doc-logits \\\n",
        "    --no-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "cKEDZ6v4n8pX"
      },
      "source": [
        "#### Student"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630227483183
        },
        "id": "Ww-NhHsnn8pX"
      },
      "source": [
        "!source activate scholar\n",
        "\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/scholar/run_scholar.py \\\n",
        "    /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/dev \\\n",
        "    --dev-metric npmi \\\n",
        "    -k 50 \\\n",
        "    --epochs 500 \\\n",
        "    --patience 500 \\\n",
        "    --batch-size 200 \\\n",
        "    --background-embeddings \\\n",
        "    --device 0 \\\n",
        "    --dev-prefix dev \\\n",
        "    -l 0.001 \\\n",
        "    --alpha 0.5 \\\n",
        "    --eta-bn-anneal-step-const 0.25 \\\n",
        "    --doc-reps-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/dev/logits/checkpoint-1320/doc_logits \\\n",
        "    --use-doc-layer \\\n",
        "    --no-bow-reconstruction-loss \\\n",
        "    --doc-reconstruction-weight 0.5 \\\n",
        "    --doc-reconstruction-temp 1.0 \\\n",
        "    --doc-reconstruction-logit-clipping 10.0 \\\n",
        "    -o ./outputs/webhose/k50_defaultParameters_new_7_epochs_lr001/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "JuzceXk2n8pX"
      },
      "source": [
        "### Articles from Corriere\n",
        "To give the chosen articles to the neural topic model we have to perform the same preprocessing steps as before, extract the logits from the teacher model and pass them to the student model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629625872593
        },
        "id": "8fs-qjZNn8pX"
      },
      "source": [
        "def save_and_preprocess_articles(corpus_path,train_dict):\n",
        "\n",
        "    \"\"\"Preprocess the test articles and save results.\n",
        "        Parameters\n",
        "        ----------\n",
        "        - corpus_path: String, the path of the .txt file containig the corpus.\n",
        "        - train_dict: Dictionary, the dictionary created during training\"\"\"\n",
        "\n",
        "    corpus = open(\n",
        "        corpus_path, encoding=\"utf-8\"\n",
        "    ).read()\n",
        "\n",
        "    corpus = corpus.split(\"</text>\")\n",
        "\n",
        "    docs = []\n",
        "    for doc in corpus:\n",
        "        try:\n",
        "            if(doc!='' and doc!='\\n'):\n",
        "                t = doc.split(\"</text\")[0]\n",
        "                if (doc!=''):\n",
        "                    docs.append(t.split(\"\\\">\")[1])\n",
        "        except IndexError:\n",
        "            print(\"doc: \" + doc)\n",
        "            print(\"previous doc: \" + docs[-1])\n",
        "    \n",
        "    test_dict, _, raw_tokens_test = preprocess_documents(docs, train_dict, language = \"italian\")\n",
        "    \n",
        "    ## Swap key and value of the dictionary\n",
        "    vocab_dict = dict([(value, key) for key, value in train_dict.items()])\n",
        "\n",
        "    ## Compute count matrix\n",
        "    raw_counts_test = np.array([toks_to_onehot(doc, vocab_dict) for doc in raw_tokens_test])\n",
        "\n",
        "    ## Filter out the zero-counts\n",
        "    nonzero_test = raw_counts_test.sum(1) > 0\n",
        "        \n",
        "    ## Keep only non empty documents\n",
        "    raw_ids_test = [idx for idx, keep in enumerate(nonzero_test) if keep]\n",
        "\n",
        "    raw_tokens_test = [' '.join(raw_tokens_test[idx]) for idx in raw_ids_test]\n",
        "\n",
        "    raw_counts_test = raw_counts_test[nonzero_test] # list of lists of 0/1\n",
        "\n",
        "    raw_data_test = [{'id': idx, 'text': docs[idx]} for idx in raw_ids_test]\n",
        "\n",
        "    ## save data\n",
        "    %cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/corriere_dataset\n",
        "    Path(\"replicated\").mkdir(exist_ok=True)\n",
        "    save_sparse(sparse.coo_matrix(raw_counts_test), \"./replicated/test.npz\")\n",
        "    keys_dict = [\n",
        "        k for k, v in vocab_dict.items()\n",
        "    ]   \n",
        "    save_json(keys_dict, \"./replicated/train.vocab.json\")\n",
        "\n",
        "    save_json(raw_tokens_test, \"./replicated/test.tokens.json\")\n",
        "\n",
        "    save_jsonlist(raw_data_test, \"./replicated/test.jsonlist\")\n",
        "\n",
        "    save_json([d['id'] for d in raw_data_test], \"./replicated/test.ids.json\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "dRtdkmczn8pX"
      },
      "source": [
        "We can apply the defined functions and scripts, similar to the ones applied on the Webhose corpus, to obtain the preprocessed version of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629627300565
        },
        "id": "RzmpwOiJn8pX"
      },
      "source": [
        "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl\n",
        "\n",
        "article_path = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/corriere_dataset/articles.txt\" \n",
        "save_and_preprocess_articles(article_path,train_dict_webhose)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "klCqDLu-n8pY"
      },
      "source": [
        "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/corriere_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "B4bDnJIQn8pY"
      },
      "source": [
        "!cp ./replicated/test.ids.json ./replicated/test.jsonlist ./replicated/test.npz ./replicated/test.tokens.json ./replicated/dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629626568252
        },
        "id": "nLCWIctKn8pY"
      },
      "source": [
        "#!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2876181/code/Users/S287618/ProjectMLDL/kd-topic-models/data/articles/4_create_dev_sets.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "gLAvEJY-n8pY"
      },
      "source": [
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/corriere_dataset/6_create_raw_text_file.py /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/corriere_dataset/replicated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "JdfHLzQWn8pY"
      },
      "source": [
        "Now that we have preprocessed dataset, we can use the trained teacher model to extract the logits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629627438234
        },
        "id": "0k2OFFBOn8pY"
      },
      "source": [
        "#!source activate transformers28\n",
        "\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/teacher/bert_reconstruction.py \\\n",
        "    --input-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/dev \\\n",
        "    --output-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/corriere_dataset/replicated/dev/logits \\\n",
        "    --test-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/corriere_dataset/replicated/dev \\\n",
        "    --seed 42 \\\n",
        "    --num-workers 4 \\\n",
        "    --checkpoint-folder-pattern \"checkpoint-1155\" \\\n",
        "    --save-doc-logits \\\n",
        "    --do-eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "7XTmRJlBn8pY"
      },
      "source": [
        "Now, we use the logits to apply the student model to the articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "SHdUGzRen8pY"
      },
      "source": [
        "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630079542826
        },
        "id": "vGCCvzU7n8pZ"
      },
      "source": [
        "!source activate scholar\n",
        "\n",
        "!python /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/scholar/run_scholar.py \\\n",
        "    /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/corriere_dataset/replicated/dev \\\n",
        "    --batch-size 200 \\\n",
        "    --background-embeddings \\\n",
        "    --device 0 \\\n",
        "    --test-prefix test \\\n",
        "    --doc-reps-dir /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/corriere_dataset/replicated/dev/logits/checkpoint-1155/doc_logits \\\n",
        "    -o ./outputs/corriere_dataset/k50_defaultParameters_new_7_epochs_lr001 \\\n",
        "    --do-test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "iesEL1Zyn8pZ"
      },
      "source": [
        "Now we can extract the most relevant topics for each article to evaluate qualitatively the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630079576154
        },
        "id": "9V4TK4hbn8pZ"
      },
      "source": [
        "def read_topic(path_topics, ind):\n",
        "    with open(path_topics, \"r\") as topics_file:\n",
        "        lines = topics_file.readlines()\n",
        "        for index,line in enumerate(lines):\n",
        "            if(index == ind):\n",
        "                words = line.split(\" \")[:9]\n",
        "                return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630079753933
        },
        "id": "YhYRAYjpn8pZ"
      },
      "source": [
        "import numpy as np\n",
        "data = np.load(\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/corriere_dataset/k50_defaultParameters_new_7_epochs_lr001/theta.test.npz\")\n",
        "fields = data.files\n",
        "path_topics = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/corriere_dataset/k50_defaultParameters_new_7_epochs_lr001/topics.txt\"\n",
        "for item in fields:\n",
        "    if(item == \"theta\"):\n",
        "        for i in range(3):\n",
        "          print(\"Document {} top topics:\".format(i+1))\n",
        "          ind = data[item][i].argsort()[-3:][::-1]\n",
        "          for el in ind:\n",
        "            value = data[item][i][el]\n",
        "            topic = read_topic(path_topics, el)\n",
        "            print(value, \": topic \", el, topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Ik0_dxWNn8pZ"
      },
      "source": [
        "### Search engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630603148015
        },
        "id": "7TkqJ2din8pZ"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from scipy.spatial import distance\n",
        "!pip install --upgrade scipy\n",
        "\n",
        "def load_jsonlist(fpath):\n",
        "    data = []\n",
        "    with open(fpath, 'r', encoding='utf-8') as i:\n",
        "        data = [json.loads(line) for line in i]\n",
        "    return data\n",
        "\n",
        "def search_engine(topic_dist, language, num_related_docs, metric):\n",
        "\n",
        "  \"\"\"Search within the training dataset the document(s) charcterized by a similar topic distribution to, or the most chacterizing topic of the one provided in input.\n",
        "    Parameters\n",
        "    ----------\n",
        "    - topic_dist: List of Floats, a distribution over topics.\n",
        "    - language: \"Italian\" or \"English\", it specifies the data set in which to search\n",
        "    - num_related_docs: Integer, number of douments to be retrieved\n",
        "    - meteric: String, \"topic\", retrieves the document(s) in which the probability, of the most relevant topic of the distribution in input, is higher;\n",
        "                       \"js\", retrieves the document(s) with the most similar topic distribution to the one given in input.\n",
        "    Output\n",
        "    ------\n",
        "    - results: List of Dictionaries, keys of each document: \"document_id\": Integer, the id of the retrieved document\n",
        "                                                            \"value\": Float, if metric = \"js\" is the value of the Jensen-Shannon divergence between the two distributions\n",
        "                                                                            if metric = \"topic\" is the proportion of the most probable topic in the retrieved document\n",
        "                                                            \"text: String, the text of the retrieved document\"\"\"\n",
        "\n",
        "  if language == \"english\":\n",
        "    theta_and_topics_path =  \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/20ng_replicated\"\n",
        "    train_jsonlist_path = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/20ng/replicated/dev\"\n",
        "  elif language == 'italian':\n",
        "    theta_and_topics_path = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/webhose/k50_defaultParameters_new_7_epochs_lr001\"\n",
        "    train_jsonlist_path = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/dev\"\n",
        "  \n",
        "  train_jsonlist = load_jsonlist(os.path.join(train_jsonlist_path,\"train.jsonlist\"))\n",
        "  theta = np.load(os.path.join(theta_and_topics_path,\"theta.train.npz\"))['theta']\n",
        "  topic_id = topic_dist.argmax()\n",
        "  \n",
        "  print(\"Most relevant topic: \", topic_id)\n",
        "  \n",
        "  if metric == \"js\":\n",
        "    topic_matrix = np.tile(topic_dist, (theta.shape[0], 1))\n",
        "    thetaT = theta.T\n",
        "    topic_matrixT = topic_matrix.T\n",
        "    ditanaces = distance.jensenshannon(thetaT, topic_matrixT)\n",
        "    most_related_document = ditanaces[ditanaces.argsort()[:num_related_docs]]\n",
        "    most_related_document_ids = ditanaces.argsort()[:num_related_docs]\n",
        "  elif metric == \"topic\":\n",
        "    most_related_document = theta[:,topic_id][theta[:,topic_id].argsort()[::-1][:num_related_docs]]\n",
        "    most_related_document_ids = theta[:,topic_id].argsort()[::-1][:num_related_docs]\n",
        "  \n",
        "  results = [{'document_id': train_jsonlist[id]['id'],'value': topic_probability, 'text': train_jsonlist[id]['text']}for id, topic_probability in zip(most_related_document_ids, most_related_document)]\n",
        "  \n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630604036894
        },
        "id": "vqXbQ98Pn8pa"
      },
      "source": [
        "theta_and_topics_path =  \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/corriere_dataset/k50_defaultParameters_new_7_epochs_lr001\"\n",
        "theta1 = np.load(os.path.join(theta_and_topics_path,\"theta.test.npz\"))['theta']\n",
        "topic_dist = theta1[0]\n",
        "search_engine(topic_dist, \"italian\", 3, \"topic\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "IaM5hPVpn8pa"
      },
      "source": [
        "### Topics alignment (Scholar / Scholar + BAT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0nad2maonwa",
        "gather": {
          "logged": 1630080327707
        }
      },
      "source": [
        "def get_npmi_vals_and_topic_words(ref_vocab, ref_counts, topics, n=10, cols_to_skip=0):\n",
        "    vocab_index = dict(zip(ref_vocab, range(len(ref_vocab))))\n",
        "    n_docs, _ = ref_counts.shape\n",
        "    npmi_values, top_words_strings = [], []\n",
        "    for topic in topics:\n",
        "        words = topic.strip().split()[cols_to_skip:]\n",
        "        npmi_vals = []\n",
        "        for word_i, word1 in enumerate(words[:n]):\n",
        "            if word1 in vocab_index:\n",
        "                index1 = vocab_index[word1]\n",
        "            else:\n",
        "                index1 = None\n",
        "            for word2 in words[word_i+1:n]:\n",
        "                if word2 in vocab_index:\n",
        "                    index2 = vocab_index[word2]\n",
        "                else:\n",
        "                    index2 = None\n",
        "                if index1 is None or index2 is None:\n",
        "                    npmi = 0.0\n",
        "                else:\n",
        "                    col1 = np.array((ref_counts[:, index1] > 0).todense(), dtype=int)\n",
        "                    col2 = np.array((ref_counts[:, index2] > 0).todense(), dtype=int)\n",
        "                    c1 = col1.sum()\n",
        "                    c2 = col2.sum()\n",
        "                    c12 = np.sum(col1 * col2)\n",
        "                    if c12 == 0:\n",
        "                        npmi = 0.0\n",
        "                    else:\n",
        "                        npmi = (np.log10(n_docs) + np.log10(c12) - np.log10(c1) - np.log10(c2)) / (np.log10(n_docs) - np.log10(c12))\n",
        "                npmi_vals.append(npmi)\n",
        "        npmi_values.append(round(np.mean(npmi_vals), 4))\n",
        "        top_words_strings.append(' '.join(words[:n]))\n",
        "    return npmi_values, top_words_strings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXaY6ELxonwa",
        "gather": {
          "logged": 1630080335347
        }
      },
      "source": [
        "def get_npmi_topics(datapath, modelpath, n=10):\n",
        "    ref_vocab = fh.read_json(datapath + 'train.vocab.json')\n",
        "    ref_counts = fh.load_sparse(datapath + 'dev.npz').tocsc()\n",
        "    out = []\n",
        "\n",
        "    topics = fh.read_text(modelpath + '/topics.txt')\n",
        "    npmi_values, top_words_strings = get_npmi_vals_and_topic_words(ref_vocab, ref_counts, topics, n)\n",
        "    out.append(list(zip(npmi_values, top_words_strings)))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "HjPRBXJCn8pb"
      },
      "source": [
        "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/scholar/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630080359934
        },
        "id": "vKxjSJEYn8pb"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from run_scholar import print_top_words\n",
        "import scipy\n",
        "import torch\n",
        "import pickle\n",
        "import file_handling as fh\n",
        "\n",
        "def jsd(p, q, base=np.e):\n",
        "    '''\n",
        "        Implementation of pairwise `jsd` based on  \n",
        "        https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
        "    '''\n",
        "    \n",
        "    ## normalize p, q to probabilities\n",
        "    p, q = np.array(torch.softmax(torch.from_numpy(p), dim=0)), np.array(torch.softmax(torch.from_numpy(q), dim=0))\n",
        "    m = (p + q)/2\n",
        "    return scipy.stats.entropy(p, m, base=base)/2. +  scipy.stats.entropy(q, m, base=base)/2.\n",
        "\n",
        "def js_divergence(beta1, beta2):\n",
        "\n",
        "    assert beta1.shape==beta2.shape\n",
        "    x, y = beta1.shape\n",
        "    js_div_score_matrix = np.zeros((x,x))\n",
        "    for i in range(x):\n",
        "        for j in range(x):\n",
        "            js_div_score_matrix[i][j] = round(jsd(beta1[i], beta2[j]), 4)\n",
        "    return js_div_score_matrix\n",
        "\n",
        "def get_topic_matched_pairs(beta1, beta2):\n",
        "    assert beta1.shape==beta2.shape\n",
        "    js_div_scores = js_divergence(beta1, beta2)\n",
        "    topic_match_tuples = []\n",
        "    topic_match_scores = []\n",
        "    while len(topic_match_tuples)<50:\n",
        "        z = np.argmin(js_div_scores) \n",
        "        i = z//js_div_scores.shape[1]\n",
        "        j = z%js_div_scores.shape[1]\n",
        "        topic_match_tuples.append((i,j))\n",
        "        topic_match_scores.append(np.min(js_div_scores))\n",
        "        js_div_scores[i, :] = 2.0\n",
        "        js_div_scores[:, j] = 2.0\n",
        "    return topic_match_tuples, topic_match_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630080429720
        },
        "id": "tTIP1zq9n8pb"
      },
      "source": [
        "beta_baseline = np.load(\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/webhose/scholar_k50_defaultParameters_lr001/beta.npz\")['beta']\n",
        "beta_kd = np.load(\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/webhose/k50_defaultParameters_new_7_epochs_lr001/beta.npz\")['beta']\n",
        "topic_pairs_jsdiv_baseline_kd, scores = get_topic_matched_pairs(beta_baseline, beta_kd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "zCK-Sdnfn8pb"
      },
      "source": [
        "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630080650726
        },
        "id": "8gSrryIOn8pc"
      },
      "source": [
        "def print_compare_baseline_kd_matched_topics(baseline_npmi_topics, kd_npmi_topics, topic_pairs_jsdiv_baseline_kd, scores, top_matched_pairs=10):\n",
        "    kd_wins, baseline_wins = 0, 0\n",
        "    topic_pairs_jsdiv_baseline_kd = topic_pairs_jsdiv_baseline_kd[:top_matched_pairs]\n",
        "    df = pd.DataFrame(columns=['Pair #', 'SCHOLAR vs SCHOLAR+BAT', 'JS Divergence'])\n",
        "    ind = list(range(1, 44+1))\n",
        "    b_k, js = [], []\n",
        "    for x, y in zip(topic_pairs_jsdiv_baseline_kd, scores):\n",
        "        print('SCHOLAR: ' + str(baseline_npmi_topics[0][x[0]]) + '\\nSCHOLAR+BAT: ' + str(kd_npmi_topics[0][x[1]]))\n",
        "        b_k.append('SCHOLAR: ' + str(baseline_npmi_topics[0][x[0]]) + '\\nSCHOLAR+BAT: ' + str(kd_npmi_topics[0][x[1]]))\n",
        "        print('JS Div. Value = ' + str(y))\n",
        "        js.append(y)\n",
        "        if baseline_npmi_topics[0][x[0]][0]>kd_npmi_topics[0][x[1]][0]:\n",
        "            baseline_wins+=1\n",
        "        else:\n",
        "            kd_wins+=1\n",
        "        print('---')\n",
        "    df['Pair #'] = ind\n",
        "    df['SCHOLAR vs SCHOLAR+BAT'] = b_k\n",
        "    df['JS Divergence'] = js\n",
        "    return df, baseline_wins, kd_wins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630080657815
        },
        "id": "C58ZEa7Qn8pc"
      },
      "source": [
        "baseline_npmi_topics = get_npmi_topics(\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/dev/\", \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/webhose/scholar_k50_defaultParameters_lr001\")\n",
        "kd_npmi_topics = get_npmi_topics(datapath=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/kd-topic-models/data/webhose/replicated/dev/\", modelpath=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/s2838321/code/Users/S283832/project_mldl/outputs/webhose/k50_defaultParameters_new_7_epochs_lr001\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630080664365
        },
        "id": "blcj76Vin8pc"
      },
      "source": [
        "df, baseline_wins, kd_wins = print_compare_baseline_kd_matched_topics(baseline_npmi_topics,kd_npmi_topics,topic_pairs_jsdiv_baseline_kd,scores,44)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630080690371
        },
        "id": "IMfs_eNPn8pc"
      },
      "source": [
        "kd_wins,baseline_wins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630080819925
        },
        "id": "IZ8rlVpyn8pc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "x = list(range(1,51))\n",
        "plt.plot(x,scores)\n",
        "plt.ylabel(\"JS divergence\")\n",
        "plt.xlabel(\"Topic pairs ordered by similarity\")\n",
        "plt.savefig('ITA_k50_js_divergence.eps', format='eps')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcE-88I4of-e"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgFoHRDvo8X5"
      },
      "source": [
        "Due to some library updates mismatch it is recommended to run this part locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRftqy9FojM_"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy import sparse\n",
        "from scipy.special import softmax\n",
        "\n",
        "def load_model(filename, docs_path):\n",
        "    beta = np.load(os.path.join(filename,\"beta.npz\"))['beta']\n",
        "    beta = softmax(beta, axis = 1)\n",
        "    theta = np.load(os.path.join(filename,\"theta.train.npz\"))['theta']\n",
        "    with open(os.path.join(docs_path,\"train.vocab.json\"), 'r') as j:\n",
        "        vocab = json.load(j)\n",
        "    train = np.load(os.path.join(docs_path,\"train.npz\"))\n",
        "    data = sparse.coo_matrix((train['data'], (train['row'], train['col'])), shape=train['shape']).todense()\n",
        "    data = np.array(data)\n",
        "    counts = np.sum(data, axis = 0, keepdims = False)\n",
        "    lengths = [] \n",
        "    with open(os.path.join(docs_path,\"train.tokens.json\"), 'r') as j:\n",
        "        tokenized_docs = json.load(j)\n",
        "        for doc in tokenized_docs:\n",
        "            lengths.append(len(doc))\n",
        "    data = {'topic_term_dists': beta, #beta\n",
        "          'doc_topic_dists': theta, #theta\n",
        "          'doc_lengths': lengths,\n",
        "          'vocab': vocab, #vocab\n",
        "          'term_frequency': counts} #term freq\n",
        "    return data\n",
        "\n",
        "docs_path = './20ng/k50'\n",
        "filename = './20ng/k50'\n",
        "model_data = load_model(filename, docs_path)\n",
        "\n",
        "print('Topic-Term shape: %s' % str(np.array(model_data['topic_term_dists']).shape))\n",
        "print('Doc-Topic shape: %s' % str(np.array(model_data['doc_topic_dists']).shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALHb1JLXopXI"
      },
      "source": [
        "!pip install pyLDAvis\n",
        "\n",
        "import pyLDAvis\n",
        "\n",
        "vis_data = pyLDAvis.prepare(**model_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yukmUYdYos28"
      },
      "source": [
        "pyLDAvis.save_html(vis_data, 'scholarbatENG.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC2T9uKPov9a"
      },
      "source": [
        "%matplotlib inline\n",
        "pyLDAvis.display(vis_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}